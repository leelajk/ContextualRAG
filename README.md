# ContextualRAG
ContextualRAG:  A Python RAG pipeline powered by LangChain, ChromaDB, and Ollama for scalable document QA and summarization.

#RAG System with LangChain, ChromaDB, and Ollama
A complete Retrieval-Augmented Generation (RAG) system built with LangChain that enables you to ask questions about any text document and receive contextually relevant answers powered by a local LLM.

#Features
1) Local-First Design - No cloud APIs, no costs, no account requirements
2) Semantic Search - Find relevant content using embeddings, not keyword matching
3) Persistent Storage - ChromaDB stores embeddings locally for fast retrieval
4) LLM Integration - Uses Mistral 7B for high-quality answers
5) Interactive Mode - Ask unlimited questions in a conversational loop
6) Well-Documented Code - Extensive comments explaining each step
7) Easy Setup - Simple installation with pip and one configuration command

#Prerequisites
System Requirements
Python 3.8 or higher

RAM: Minimum 8GB (16GB+ recommended for smooth Ollama performance)

Disk Space: ~10GB for Mistral 7B model

Internet: Req

